{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:02:00.740606Z",
     "start_time": "2024-05-12T15:02:00.736793Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the dataset with random preferences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ee46dd8ecd04023"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./fashion/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 13441076.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./fashion/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./fashion/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./fashion/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 150806.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./fashion/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./fashion/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./fashion/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:05<00:00, 750132.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./fashion/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./fashion/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./fashion/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 6811443.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./fashion/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./fashion/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNISTWithPreferences/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 14070717.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNISTWithPreferences/raw/train-images-idx3-ubyte.gz to ./data/FashionMNISTWithPreferences/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNISTWithPreferences/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 268418.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNISTWithPreferences/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNISTWithPreferences/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNISTWithPreferences/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:01<00:00, 3038712.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNISTWithPreferences/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNISTWithPreferences/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNISTWithPreferences/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 5031991.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNISTWithPreferences/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNISTWithPreferences/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Initialize dataset with preferences\n",
    "train_dataset = datasets.FashionMNIST(root='./fashion', train=True, download=True, transform=transform)\n",
    "\n",
    "# Define user preferences (example)\n",
    "preferences = torch.randint(0, 3, (len(train_dataset),))  # Random preferences (0: casual, 1: formal, 2: sporty)\n",
    "\n",
    "# Modify the data loading process to include preferences\n",
    "class FashionMNISTWithPreferences(datasets.FashionMNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, preferences=None):\n",
    "        super(FashionMNISTWithPreferences, self).__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "        self.preferences = preferences\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(FashionMNISTWithPreferences, self).__getitem__(index)\n",
    "        if self.preferences is not None:\n",
    "            return img, target, self.preferences[index]\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "# Initialize dataset with preferences\n",
    "train_dataset = FashionMNISTWithPreferences(root='./data', train=True, download=True, transform=transform, preferences=preferences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T14:39:06.474704Z",
     "start_time": "2024-05-12T14:38:50.223103Z"
    }
   },
   "id": "9f005b242bed97f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CVAE model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a21246dcfca18d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, image_size, preference_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size + preference_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu = nn.Linear(256, latent_dim)\n",
    "        self.logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + preference_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),  # Adjusted size here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, image_size),  \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, preferences):\n",
    "        # Flatten the input images\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Convert preferences to one-hot encoding\n",
    "        preferences_onehot = F.one_hot(preferences, num_classes=preference_size).float()\n",
    "        \n",
    "        # Concatenate flattened images with preferences\n",
    "        x = torch.cat((x, preferences_onehot), dim=1)\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(torch.cat((z, preferences_onehot), dim=1)), mu, logvar\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T14:45:03.212173Z",
     "start_time": "2024-05-12T14:45:03.208601Z"
    }
   },
   "id": "5c88a59e8542a3ce"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "latent_dim = 20\n",
    "image_size = 784  # 28x28\n",
    "preference_size = 3  # Number of preference categories\n",
    "\n",
    "model = CVAE(latent_dim, image_size, preference_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T14:45:03.927374Z",
     "start_time": "2024-05-12T14:45:03.918528Z"
    }
   },
   "id": "d635cf7afe77cf3e"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3815\n",
      "Epoch [2/10], Loss: 0.3563\n",
      "Epoch [3/10], Loss: 0.3622\n",
      "Epoch [4/10], Loss: 0.3349\n",
      "Epoch [5/10], Loss: 0.3669\n",
      "Epoch [6/10], Loss: 0.3333\n",
      "Epoch [7/10], Loss: 0.3462\n",
      "Epoch [8/10], Loss: 0.3170\n",
      "Epoch [9/10], Loss: 0.3786\n",
      "Epoch [10/10], Loss: 0.3211\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels, preferences) in enumerate(train_loader):\n",
    "        images = images.view(images.size(0), -1)\n",
    "        recon_images, mu, logvar = model(images, preferences)\n",
    "\n",
    "        recon_loss = criterion(recon_images, images)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        loss = recon_loss + kl_divergence\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T14:45:57.592566Z",
     "start_time": "2024-05-12T14:45:05.778753Z"
    }
   },
   "id": "a8df8f07bb30c6cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bf54927c43f14cb"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images saved successfully.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "latent_samples = torch.randn(64, latent_dim)\n",
    "random_preferences = torch.randint(0, preference_size, (64,))\n",
    "preferences_onehot = F.one_hot(random_preferences, num_classes=preference_size).float()\n",
    "inputs = torch.cat((latent_samples, preferences_onehot), dim=1)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    generated_images = model.decoder(inputs)\n",
    "\n",
    "# Set directory to save images\n",
    "save_dir = \"generated_images\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for j in range(preference_size):\n",
    "    latent_sample = torch.randn(1, latent_dim)\n",
    "    preference = torch.tensor([j])\n",
    "    preference_onehot = F.one_hot(preference, num_classes=preference_size).float()\n",
    "    input_ = torch.cat((latent_sample, preference_onehot), dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_image = model.decoder(input_)\n",
    "    \n",
    "    image_vector = generated_image[0].cpu().detach().numpy()\n",
    "    image_array = image_vector.reshape(28, 28)\n",
    "    image_uint8 = ((image_array + 1) / 2 * 255).astype(np.uint8)\n",
    "    image_pil = Image.fromarray(image_uint8, mode='L')  # 'L' mode for grayscale\n",
    "    image_pil.save(os.path.join(save_dir, f\"image_{j}.png\"))\n",
    "\n",
    "print(\"Images saved successfully.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:59:31.757352Z",
     "start_time": "2024-05-12T15:59:31.698312Z"
    }
   },
   "id": "d48c436a6f7b18f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
